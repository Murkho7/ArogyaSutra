ArogyaSutra: A Multi-Agent Framework for Multimodal Medical Reasoning in
Indic Languages

Multimodal Large Language1
Models (MLLMs) have shown promising reason-2
ing capabilities in general domains, yet their per-3
formance remains limited in specialized settings4
such as healthcare, particularly for multilingual and5
low-resource scenarios. This gap is critical in re-6
gions like rural India, where patients often express7
complex medical queries in native Indic languages8
and rely on multimodal inputs such as medical im-9
ages. Existing MLLMs, predominantly trained on10
English-centric data, struggle to support such use11
cases, limiting equitable access to AI-driven health-12
care assistance. To address this challenge, we con-13
struct a large-scale multilingual multimodal med-14
ical question–answer dataset from eight heteroge-15
neous sources, covering 31 body systems, six imag-16
ing modalities, and 21 clinical domains across En-17
glish and seven major Indian languages.We fur-18
ther propose ArogyaSutra, an actor–critic–based19
multi-agent framework that combines tool ground-20
ing with dual-memory mechanisms to support step-21
wise, reasoning-aware decision making while ex-22
plicitly retaining past mistakes to prevent their re-23
peated occurrence. The Actor predicts high-level24
semantic actions from visual and system states,25
whereas the Critic evaluates action outcomes and26
delivers corrective feedback, enabling iterative re-27
finement of the reasoning process. Experiments28
show that our dataset and framework improve the29
medical reasoning accuracy of an MLLM by 9.18%30
on average across Indic languages, with ablation31
studies validating the effectiveness of each com-32
ponent. Our work advances inclusive multilingual33
multimodal medical reasoning, contributing toward34
socially beneficial AI for underserved communi-35
ties.
